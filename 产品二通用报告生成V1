# -*- coding: utf-8 -*-
import pandas as pd
import glob
import os
import re
from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment, PatternFill, Font
from openpyxl.formatting.rule import DataBarRule, ColorScaleRule
from openpyxl.chart import BarChart, Reference, LineChart
from openpyxl.chart.axis import DateAxis
from datetime import datetime, timedelta

# --- 1. 配置区 ---
CONFIG = {
    "PLATFORM_NAME": "vnl平台",
    "OUTPUT_FILENAME": "final_report_v5.xlsx"
}

# --- 2. 各Sheet的最终列名定义 ---
BACKEND_TARGET_COLUMNS = [
    '平台', '时间', '新增人员', '活跃人数', '充值人数', '商城充值次数', '总赠送', '商城充值金额', '实际到账金币', '笔次均值', '人均充值', '提现人数', '商城提现次数', '商城提现金额', '实际提现金额', 'ARPU', '当日总流水', '当日总返奖', '当日总盈利', '充值投注人数', '新增充值人数', '新增绑定比', '今日投充比', '首充次日复投率', '首充3日复投率', '首充7日复投率', '首充用户总充值金额', 'PWA数量', '投注人数', '一级新充人数', '一级新充金额', '一级首充人数', '一级首充金额', '新充金额', '首充人数', '首笔汇总', '充提差'
]
CHANNEL_TARGET_COLUMNS = [
    '首充金额', '日期', '首充', '一级首充', '非首充', '新增人员', '新增充值', '一级新充', '裂变占比', '新增/首充率', '老用户充值', '活跃充值', '提现人数', '提现金额', '提现率', '首充提现率(人数)', '首充提现率(金额)', '老用户付费率', '老用户付费占比', '活跃付费率', 'ARPU'
]
RETENTION_COLUMNS = ['时间', '首充人数', '次日', '3日', '4日', '5日', '6日', '7日', '8日', '9日', '10日', '11日', '12日', '13日', '14日', '15日', '16日', '17日', '18日', '19日', '20日', '21日', '22日', '23日', '24日', '25日', '26日', '27日', '28日', '29日', '30日']

def format_channel_retention_sheet(writer, files, sheet_name, summary_days, first_pay_mapping=None):
    """格式化渠道留存sheet"""
    workbook = writer.book
    ws = workbook.create_sheet(title=sheet_name)
    
    # 按截图格式设置表头
    # 第一行：时间、首充人数、留存率（合并到7日）、方式、总代号、时间、首充人数、留存率（合并到30日）
    ws.cell(row=1, column=1).value = '时间'
    ws.cell(row=1, column=2).value = '首充人数'
    ws.merge_cells(start_row=1, start_column=3, end_row=1, end_column=6)  # 留存率合并到7日
    ws.cell(row=1, column=3).value = '留存率'
    ws.cell(row=1, column=7).value = '方式'
    ws.cell(row=1, column=8).value = '总代号'
    ws.cell(row=1, column=9).value = '时间'
    ws.cell(row=1, column=10).value = '首充人数'
    ws.merge_cells(start_row=1, start_column=11, end_row=1, end_column=17)  # 留存率合并到30日
    ws.cell(row=1, column=11).value = '留存率'
    
    # 第二行：具体列名
    # 左侧：时间、首充人数、次日、3日、7日、30日、方式、总代号
    # 右侧：时间、首充人数、次日、3日、4日、5日、6日、7日、30日
    left_headers = ['时间', '首充人数', '次日', '3日', '7日', '30日', '方式', '总代号']
    right_headers = ['时间', '首充人数', '次日', '3日', '4日', '5日', '6日', '7日', '30日']
    
    for i, h in enumerate(left_headers, 1):
        ws.cell(row=2, column=i).value = h
    for i, h in enumerate(right_headers, 9):
        ws.cell(row=2, column=i).value = h

    # 按总代号分组处理文件
    agent_groups = {}
    for fp in files:
        try:
            basename = os.path.basename(fp)
            print(f"  Processing file: {basename}")
            # 提取总代号：download_first_login_return_1102 _ TL_333_XXX_AAA_DP_本月_2025-10-16
            # 或者：用户留存率_首充复登_1102 _ TL_333_XXX_AAA_DP_本月_2025-10-21
            # 或者：用户留存率_首充复充_1102 _ TL_333_XXX_AAA_DP_本月_2025-10-21
            m = re.search(r'(?:download_first_(?:login|recharge)_return_|用户留存率_首充(?:复登|复充)_)(\d+)\s*_', basename)
            if m:
                agent_code = m.group(1)
                if agent_code not in agent_groups:
                    agent_groups[agent_code] = []
                agent_groups[agent_code].append(fp)
        except Exception as e:
            print(f"    Error processing {basename}: {e}")
    
    # 处理每个总代号的数据
    row = 3
    for agent_code, agent_files in agent_groups.items():
        print(f"  Processing agent {agent_code} with {len(agent_files)} files")
        
        # 获取总代号对应的渠道信息
        channel_info = None
        for rule in CHANNEL_RULES:
            if rule['代码'] == agent_code:
                channel_info = rule
                break
        
        if not channel_info:
            print(f"    Warning: No channel info found for agent {agent_code}")
            continue
            
        # 处理每个文件的数据
        for file_path in agent_files:
            try:
                df = pd.read_csv(file_path, encoding='utf-8')
                
                # 过滤掉数据汇总行
                if '时间' in df.columns:
                    df = df[~df['时间'].astype(str).str.contains('数据汇总|总计', na=False, regex=True)].copy()
                
                print(f"    Processing file: {os.path.basename(file_path)}")
                print(f"    Data shape after filtering: {df.shape}")
                
                # 处理每一行数据
                for _, data_row in df.iterrows():
                    # 左侧数据：时间、首充人数、次日、3日、7日、30日、方式、总代号
                    ws.cell(row=row, column=1).value = data_row['时间']
                    ws.cell(row=row, column=2).value = data_row['首充人数']
                    ws.cell(row=row, column=3).value = data_row['次日']
                    ws.cell(row=row, column=4).value = data_row['3日']
                    ws.cell(row=row, column=5).value = data_row['7日']
                    ws.cell(row=row, column=6).value = data_row['30日']
                    ws.cell(row=row, column=7).value = channel_info['分类']
                    ws.cell(row=row, column=8).value = agent_code  # 显示总代号（数字），如1102, 1103
                    
                    # 右侧数据：时间、首充人数、次日、3日、4日、5日、6日、7日、30日
                    ws.cell(row=row, column=9).value = data_row['时间']
                    ws.cell(row=row, column=10).value = data_row['首充人数']
                    ws.cell(row=row, column=11).value = data_row['次日']
                    ws.cell(row=row, column=12).value = data_row['3日']
                    ws.cell(row=row, column=13).value = data_row.get('4日', '')
                    ws.cell(row=row, column=14).value = data_row.get('5日', '')
                    ws.cell(row=row, column=15).value = data_row.get('6日', '')
                    ws.cell(row=row, column=16).value = data_row['7日']
                    ws.cell(row=row, column=17).value = data_row['30日']
                    
                    row += 1
                    
            except Exception as e:
                print(f"    Error processing file {file_path}: {e}")
                continue

CHANNEL_RULES = [
    {'代码': '1102', '名称': 'TL_333_XXX_AAA_DP', '分类': '短信'},
    {'代码': '1103', '名称': 'TL_333_XXX_AAA_JG', '分类': '短信'},
    {'代码': '1104', '名称': 'TL_111_KKK_BBB_A8', '分类': '投放'},
    {'代码': '1105', '名称': 'TL_111_KKK_BBB_TF', '分类': '投放'},
    {'代码': '1106', '名称': 'TL_111_KKK_BBB_XN', '分类': '投放'},
    {'代码': '1107', '名称': 'TL_333_XXX_AAA_A81', '分类': '短信'},
    {'代码': '1108', '名称': 'TL_111_KKK_BBB_A82', '分类': '投放'},
    {'代码': '1109', '名称': 'TL_222_III_AAA_HZ', '分类': '网红'},
    {'代码': '1110', '名称': 'TL_111_GGG_AAA_A8', '分类': '投放'},
    {'代码': '1111', '名称': 'TL_111_KKK_BBB_OK', '分类': '投放'},
    {'代码': '1112', '名称': 'TL_111_QQQ_AAA_A8SEO', '分类': '投放'},
    {'代码': '1113', '名称': 'TL_333_XXX_AAA_DX2', '分类': '短信'},
]

# --- 3. Sheet生成与格式化函数 ---

# <--- 此函数已被重写，以解决核心逻辑问题
def process_cumulative_files(files, date_col_name, sort_by_col=None, existing_data=None, is_incremental=False):
    """
    一个更健壮的函数，用于处理随时间累积的报表文件。
    支持增量更新模式：保留历史数据，只处理新数据。
    """
    if not files: 
        print(f"  Warning: No files found")
        return None
    
    try:
        print(f"  Processing {len(files)} files...")
        df = pd.concat([pd.read_csv(f, sep=',', engine='python', on_bad_lines='skip', encoding='utf-8') for f in files], ignore_index=True)
        
        df.columns = df.columns.str.strip()
        print(f"  File columns: {len(df.columns)} columns")
        
        if date_col_name not in df.columns: 
            print(f"  Error: Cannot find date column '{date_col_name}'")
            return None
        
        df = df[~df[date_col_name].astype(str).str.contains('数据汇总|总计', na=False, regex=True)].copy()
        df[date_col_name] = pd.to_datetime(df[date_col_name], errors='coerce')
        df.dropna(subset=[date_col_name], inplace=True)

        if df.empty:
            print(f"  Warning: Data is empty after filtering")
            return None
        
        # 增量更新逻辑：如果是增量模式且有现有数据，只保留新数据
        if is_incremental and existing_data:
            df['时间_str'] = df[date_col_name].astype(str)
            # 只保留不在现有数据中的新日期
            new_data = df[~df['时间_str'].isin(existing_data)]
            if len(new_data) > 0:
                print(f"  Found {len(new_data)} new data rows (incremental update)")
                df = new_data.drop('时间_str', axis=1)
            else:
                print(f"  No new data, skipping this file group")
                return None

        # 核心逻辑：在去重前，先按日期和数据完整性排序
        # 如果指定了 sort_by_col，则按其排序；否则自动计算“留存列的完整度”
        sort_columns = [date_col_name]
        if sort_by_col and sort_by_col in df.columns:
            sort_columns.append(sort_by_col)
            print(f"  Using column '{sort_by_col}' for data integrity sorting")
            df.sort_values(by=sort_columns, ascending=True, na_position='first', inplace=True)
        else:
            # 自动识别留存相关列（包含“日”且不是日期列）
            retention_like_cols = [c for c in df.columns if c != date_col_name and ('日' in str(c) or re.match(r'^[Dd](\d+)', str(c)))]
            if retention_like_cols:
                completeness_col = '__completeness__'
                df[completeness_col] = df[retention_like_cols].notna().sum(axis=1)
                print(f"  Sorting by retention column completeness, columns: {len(retention_like_cols)}")
                # 低完整度在前，高完整度在后；去重时保留最后一条（最完整）
                df.sort_values(by=[date_col_name, completeness_col], ascending=[True, True], na_position='first', inplace=True)
            else:
                print(f"  Sort column '{sort_by_col}' not found, sorting by date only")
        df.sort_values(by=sort_columns, ascending=True, na_position='first', inplace=True)

        # 现在去重，可以安全地保留最后一条（即最完整）的记录
        original_count = len(df)
        df.drop_duplicates(subset=[date_col_name], keep='last', inplace=True)
        if '__completeness__' in df.columns:
            df.drop(columns='__completeness__', inplace=True)
        print(f"  Before deduplication: {original_count} rows, after: {len(df)} rows")
        
        return df
    except Exception as e:
        print(f"  Error processing files {files}: {e}")
        import traceback
        traceback.print_exc()
        return None


def generate_backend_sheet(data_files, existing_data=None, is_incremental=False):
    # 使用新的健壮的处理函数
    df = process_cumulative_files(data_files, '时间', sort_by_col=None, existing_data=existing_data, is_incremental=is_incremental)
    if df is None or df.empty: return None
    
    df = df.sort_values(by='时间').reset_index(drop=True)
    final_df = pd.DataFrame()
    for col in BACKEND_TARGET_COLUMNS:
        final_df[col] = df[col] if col in df.columns else None
    final_df['平台'] = CONFIG["PLATFORM_NAME"]
    final_df['时间'] = final_df['时间'].dt.strftime('%#d/%#m/%Y')
    return final_df[BACKEND_TARGET_COLUMNS]


def generate_channel_sheet(quality_files, user_files, economic_files):
    print(f"  Processing promotion channel statistics, file count: q={len(quality_files)}, u={len(user_files)}, e={len(economic_files)}")
    
    if not any([quality_files, user_files, economic_files]): 
        print("  Promotion channel statistics files completely missing")
        return None
    
    try:
        # 处理所有文件，而不是只处理第一个
        all_data = []
        
        # 处理渠道质量文件
        for file in quality_files:
            try:
                df = pd.read_csv(file, sep=',', engine='python', on_bad_lines='skip', encoding='utf-8')
                df['文件类型'] = '渠道质量'
                all_data.append(df)
                print(f"    Channel quality file: {os.path.basename(file)} - {df.shape}")
            except Exception as e:
                print(f"    Error processing channel quality file: {e}")
        
        # 处理用户质量文件
        for file in user_files:
            try:
                df = pd.read_csv(file, sep=',', engine='python', on_bad_lines='skip', encoding='utf-8')
                df['文件类型'] = '用户质量'
                all_data.append(df)
                print(f"    User quality file: {os.path.basename(file)} - {df.shape}")
            except Exception as e:
                print(f"    Error processing user quality file: {e}")
        
        # 处理经济效益文件
        for file in economic_files:
            try:
                df = pd.read_csv(file, sep=',', engine='python', on_bad_lines='skip', encoding='utf-8')
                df['文件类型'] = '经济效益'
                all_data.append(df)
                print(f"    经济效益文件: {os.path.basename(file)} - {df.shape}")
            except Exception as e:
                print(f"    处理经济效益文件出错: {e}")
        
        if not all_data:
            print("  没有成功加载任何数据")
            return None
        
        # 合并所有数据
        merged = pd.concat(all_data, ignore_index=True)
        print(f"    合并后数据: {merged.shape}")
        print(f"    列名: {list(merged.columns)}")

        # 清理列名，去除多余的空格和特殊字符
        merged.columns = merged.columns.str.strip()
        # 处理可能的重复列名问题
        merged.columns = [col.replace('  ', ' ') for col in merged.columns]

        # 清洗数据
        if '日期' in merged.columns:
            merged = merged[~merged['日期'].astype(str).str.contains('数据汇总|总计', na=False, regex=True)].copy()
            merged['日期'] = pd.to_datetime(merged['日期'], errors='coerce')
            merged.dropna(subset=['日期'], inplace=True)
        
        # 处理特殊字段
        if '首充' in merged.columns:
            merged['首充金额'] = merged['首充'].astype(str).str.split(' | ', expand=True).get(0)
        if '新增人员_u' in merged.columns:
            merged['新增人员'] = merged['新增人员_u']
            
        # 创建最终DataFrame
        final_df = pd.DataFrame()
        for col in CHANNEL_TARGET_COLUMNS:
            if col in merged.columns:
                final_df[col] = merged[col]
            else:
                final_df[col] = None
                print(f"    警告: 列 '{col}' 不存在于数据中")
        
        if not final_df.empty:
            # 过滤掉空行：只保留有首充数据的行
            final_df = final_df.dropna(subset=['首充'], how='all')
            # 进一步过滤：确保有实际数据（不是全NaN）
            final_df = final_df[final_df['首充'].notna()]
            
            final_df = final_df.sort_values(by='日期').reset_index(drop=True)
            final_df['日期'] = final_df['日期'].dt.strftime('%#d/%#m/%Y')
            print(f"    最终数据: {final_df.shape}")
            return final_df[CHANNEL_TARGET_COLUMNS]
        else:
            print("  最终数据为空")
            return None
            
    except Exception as e:
        print(f"  Error processing 'Promotion Channel Statistics' sheet: {e}")
        return None

def format_retention_sheet(writer, df, sheet_name, summary_cols):
    workbook = writer.book
    worksheet = workbook.create_sheet(title=sheet_name)
    summary_headers = ['时间', '首充人数', '留存率']
    for c_idx, val in enumerate(summary_headers, 1):
        worksheet.cell(row=1, column=c_idx).value = val
    worksheet.merge_cells(start_row=1, start_column=3, end_row=1, end_column=2 + len(summary_cols))
    for c_idx, val in enumerate(summary_cols, 3):
        worksheet.cell(row=2, column=c_idx).value = val
    detail_start_col = 5 + len(summary_cols)
    detail_headers = ['时间', '首充人数', '留存率']
    for c_idx, val in enumerate(detail_headers, detail_start_col):
        worksheet.cell(row=1, column=c_idx).value = val
    detail_retention_cols = [c for c in RETENTION_COLUMNS if c not in ['时间', '首充人数']]
    worksheet.merge_cells(start_row=1, start_column=detail_start_col + 2, end_row=1, end_column=detail_start_col + 1 + len(detail_retention_cols))
    for c_idx, val in enumerate(detail_retention_cols, detail_start_col + 2):
        worksheet.cell(row=2, column=c_idx).value = val
    df_columns = list(df.columns)
    for r_idx, (_, row) in enumerate(df.iterrows(), 3):
        time_val = row.get('时间', '')
        first_pay_count = row.get('首充人数', '')
        worksheet.cell(row=r_idx, column=1).value = time_val
        worksheet.cell(row=r_idx, column=2).value = first_pay_count
        for c_idx, col_name in enumerate(summary_cols, 3):
            if col_name in df_columns:
                worksheet.cell(row=r_idx, column=c_idx).value = row.get(col_name, '')
        worksheet.cell(row=r_idx, column=detail_start_col).value = time_val
        worksheet.cell(row=r_idx, column=detail_start_col + 1).value = first_pay_count
        for c_idx, col_name in enumerate(detail_retention_cols, detail_start_col + 2):
            if col_name in df_columns:
                worksheet.cell(row=r_idx, column=c_idx).value = row.get(col_name, '')

def _normalize_retention_columns(df):
    """
    规范留存相关列名：
    - 将常见的日期列名别名统一为 '时间'
    - 将 D7/第7日/7天/7日留存 等统一为 '7日'；D1/次日/1日 -> '次日'
    """
    cols = list(df.columns)
    # 先去除列名中的首尾空格，并生成无空格的对照，处理诸如 '8 日' / '17 日'
    df.columns = df.columns.map(lambda x: str(x).strip())
    cols_no_space = {c: re.sub(r"\s+", "", str(c)) for c in df.columns}
    # 如果无空格名不同，则先重命名为去空格版本，避免后续匹配失败
    space_strip_map = {orig: no_space for orig, no_space in cols_no_space.items() if orig != no_space}
    if space_strip_map:
        df.rename(columns=space_strip_map, inplace=True)
        cols = list(df.columns)
    # 标准化日期列
    if '时间' not in df.columns:
        for alt in ['日期', '统计日期', 'day', 'date', 'Date']:
            if alt in df.columns:
                df.rename(columns={alt: '时间'}, inplace=True)
                break

    # 构建列名映射
    col_map = {}
    for c in cols:
        cs = str(c).strip()
        # 跳过已标准列
        if cs in ['时间', '首充人数', '首次人数'] or ('日' in cs and cs.endswith('日')) or cs == '次日':
            continue
        # 匹配 Dn / 第n日 / n日 / n天 / n日留存 / Dn留存
        m = re.match(r'^[Dd]?(?:第)?(\d{1,2})[日天]?', cs)
        if m:
            n = int(m.group(1))
            if n == 1:
                target = '次日'
            elif 3 <= n <= 30:
                target = f'{n}日'
            else:
                continue
            col_map[c] = target
            continue
        # 处理明确英文 Dn 格式
        m2 = re.match(r'^[Dd](\d{1,2})', cs)
        if m2:
            n = int(m2.group(1))
            if n == 1:
                target = '次日'
            elif 3 <= n <= 30:
                target = f'{n}日'
            else:
                continue
            col_map[c] = target
            continue
        # 其它常见别名
        if cs in ['次留', '次日留存', 'D1']:
            col_map[c] = '次日'

    if col_map:
        df.rename(columns=col_map, inplace=True)

    # 统一首充人数列名
    if '首充人数' not in df.columns and '首次人数' in df.columns:
        df.rename(columns={'首次人数': '首充人数'}, inplace=True)

    return df

def generate_retention_data(data_files):
    # 改进：不使用sort_by_col，避免因列不存在导致的问题
    df = process_cumulative_files(data_files, '时间')
    if df is None or df.empty: 
        print("  Warning: Retention rate data file is empty or cannot be read")
        return None

    print(f"  Found retention rate data, {len(df)} rows")
    print(f"  Column names: {len(df.columns)} columns")

    # 规范列名
    df = _normalize_retention_columns(df)
    if '时间' not in df.columns:
        print("  Error: Retention data missing '时间' column")
        return None

    def extract_percentage(x):
        if pd.isna(x) or x == '':
            return ''
        s = str(x)
        match = re.search(r'(\d+\.?\d*%)', s)
        if match:
            return match.group(1)
        return x
    
    # 处理包含'日'的列，提取百分比
    for col in df.columns:
        if '日' in str(col) and col not in ['时间']: 
            df[col] = df[col].apply(extract_percentage)

    # 处理可能的列名变体
    if '首充人数' not in df.columns and '首次人数' in df.columns:
        df.rename(columns={'首次人数': '首充人数'}, inplace=True)
    elif '首充人数' not in df.columns and '首充' in df.columns:
        # 如果只有'首充'列，尝试从中提取人数
        df['首充人数'] = df['首充'].apply(lambda x: str(x).split('|')[1].strip() if '|' in str(x) else x)
        
    df = df.sort_values(by='时间').reset_index(drop=True)
    df['时间'] = df['时间'].dt.strftime('%#d/%#m/%Y')
    
    print(f"  Processed retention rate data shape: {df.shape}")
    return df

def generate_consumption_data(ltv_files, rules):
    """从LTV文件中提取消耗数据，按映射规则分类"""
    if not ltv_files: return None
    try:
        print(f"  Processing consumption data from LTV files, file count: {len(ltv_files)}")
        
        consumption_data = []
        
        # 按映射规则处理每个LTV文件
        for file in ltv_files:
            try:
                df = pd.read_csv(file, encoding='utf-8')
                basename = os.path.basename(file)
                print(f"    Processing LTV file: {basename}")
                
                # 从文件名提取渠道代码
                import re
                match = re.search(r'download_ltv_(\d+)', basename)
                if not match:
                    print(f"      Could not extract channel code from {basename}")
                    continue
                    
                channel_code = match.group(1)
                
                # 根据映射规则找到对应的分类
                channel_category = None
                for rule in rules:
                    if rule['代码'] == channel_code:
                        channel_category = rule['分类']
                        break
                
                if not channel_category:
                    print(f"      No mapping found for channel code {channel_code}")
                    continue
                
                print(f"      Channel {channel_code} -> {channel_category}")
                
                # 处理该文件的所有数据行
                for _, row in df.iterrows():
                    if '消耗' in df.columns and '时间' in df.columns:
                        consumption_value = pd.to_numeric(str(row['消耗']).replace('$', ''), errors='coerce')
                        if pd.notna(consumption_value) and consumption_value > 0:
                            consumption_data.append({
                                "日期": row['时间'],
                                "渠道名称": channel_category,
                                "当日消耗": consumption_value
                            })
                            print(f"      {channel_category} consumption: {consumption_value} for date: {row['时间']}")
                            
            except Exception as e:
                print(f"      Error processing file {basename}: {e}")
                continue

        if not consumption_data: 
            print("  No consumption data found")
            return None
        
        print(f"  Found {len(consumption_data)} consumption data entries")
        
        # 按日期和渠道分类汇总消耗数据
        df = pd.DataFrame(consumption_data)
        if not df.empty:
            # 按日期和渠道名称分组，汇总消耗
            grouped = df.groupby(['日期', '渠道名称'])['当日消耗'].sum().reset_index()
            
            # 透视表：日期为行，渠道分类为列
            pivot_df = grouped.pivot(index='日期', columns='渠道名称', values='当日消耗').fillna(0)
            
            # 添加总计列
            pivot_df['总计'] = pivot_df.sum(axis=1)
            
            print(f"  Consumption data grouped by category:")
            print(f"    Categories: {list(pivot_df.columns)}")
            print(f"    Date range: {len(pivot_df)} days")
            
            return pivot_df
        
        return None

    except Exception as e:
        print(f"  Error processing consumption sheet: {e}")
        return None

def format_platform_ltv_sheet(writer, df, sheet_name):
    """格式化LTV(全平台) sheet，创建简化的两行表头结构"""
    workbook = writer.book
    worksheet = workbook.create_sheet(title=sheet_name)
    
    # 第一行：主要分类标题（合并单元格）
    headers_row1 = [
        '时间', '消耗', '新增用户', '新增用户单价', '首充人数', 
        '14日LTV', '', '30日LTV', '', '60日LTV', ''
    ]
    
    # 第二行：具体列名
    headers_row2 = [
        '时间', '消耗', '新增用户', '新增用户单价', '首充人数',
        '金额', '付费用户数', '金额', '付费用户数', '金额', '付费用户数'
    ]
    
    # 写入第一行标题
    for col, header in enumerate(headers_row1, 1):
        worksheet.cell(row=1, column=col).value = header
    
    # 写入第二行标题
    for col, header in enumerate(headers_row2, 1):
        worksheet.cell(row=2, column=col).value = header
    
    # 合并LTV相关的单元格（第一行）
    # 14日LTV (列6-7)
    worksheet.merge_cells(start_row=1, start_column=6, end_row=1, end_column=7)
    # 30日LTV (列8-9)
    worksheet.merge_cells(start_row=1, start_column=8, end_row=1, end_column=9)
    # 60日LTV (列10-11)
    worksheet.merge_cells(start_row=1, start_column=10, end_row=1, end_column=11)
    
    # 写入数据
    # 全平台LTV源文件可能列名结构：
    # 时间, 消耗, 新增用户, 新增用户单价, 首充人数, 14日LTV, 30日LTV, 60日LTV, 金额, 付费用户数, 金额.1, 付费用户数.1, 金额.2, 付费用户数.2
    # 或者可能有其他结构，需要灵活处理
    
    for row_idx, (_, row_data) in enumerate(df.iterrows(), 3):
        # 基础列
        worksheet.cell(row=row_idx, column=1).value = row_data.get('时间', '')
        worksheet.cell(row=row_idx, column=2).value = row_data.get('消耗', '')
        worksheet.cell(row=row_idx, column=3).value = row_data.get('新增用户', '')
        worksheet.cell(row=row_idx, column=4).value = row_data.get('新增用户单价', '')
        worksheet.cell(row=row_idx, column=5).value = row_data.get('首充人数', '')
        
        # 尝试多种可能的列名映射
        # 14日LTV数据 - 可能是第一对金额/付费用户数
        worksheet.cell(row=row_idx, column=6).value = row_data.get('金额', row_data.get('14日LTV金额', ''))
        worksheet.cell(row=row_idx, column=7).value = row_data.get('付费用户数', row_data.get('14日LTV付费用户数', ''))
        
        # 30日LTV数据 - 可能是第二对
        worksheet.cell(row=row_idx, column=8).value = row_data.get('金额.1', row_data.get('30日LTV金额', ''))
        worksheet.cell(row=row_idx, column=9).value = row_data.get('付费用户数.1', row_data.get('30日LTV付费用户数', ''))
        
        # 60日LTV数据 - 可能是第三对
        worksheet.cell(row=row_idx, column=10).value = row_data.get('金额.2', row_data.get('60日LTV金额', ''))
        worksheet.cell(row=row_idx, column=11).value = row_data.get('付费用户数.2', row_data.get('60日LTV付费用户数', ''))

def format_ltv_sheet_with_headers(writer, df, sheet_name):
    """格式化LTV sheet，创建两行表头结构，正确处理源文件的列名"""
    print(f"  [DEBUG] format_ltv_sheet_with_headers called for '{sheet_name}'")
    print(f"  [DEBUG] DataFrame shape: {df.shape}")
    print(f"  [DEBUG] DataFrame columns ({len(df.columns)}): {list(df.columns)[:15]}...")
    
    workbook = writer.book
    worksheet = workbook.create_sheet(title=sheet_name)
    
    # 第一行：主要分类标题（合并单元格）
    headers_row1 = [
        '时间', '消耗', '新增用户', '新增用户单价', '首充人数', '付费率', 
        '注册复登', '首充复登', '3日注册复登', '3日首充复登', '首充成本',
        '1日LTV', '', '2日LTV', '', '3日LTV', '', '7日LTV', '', 
        '14日LTV', '', '30日LTV', '', '60日LTV', ''
    ]
    
    # 第二行：具体列名
    headers_row2 = [
        '时间', '消耗', '新增用户', '新增用户单价', '首充人数', '付费率',
        '注册复登', '首充复登', '3日注册复登', '3日首充复登', '首充成本',
        '金额', '付费用户数', '金额', '付费用户数', '金额', '付费用户数', 
        '金额', '付费用户数', '金额', '付费用户数', '金额', '付费用户数', 
        '金额', '付费用户数'
    ]
    
    # 写入第一行标题
    for col, header in enumerate(headers_row1, 1):
        worksheet.cell(row=1, column=col).value = header
    
    # 写入第二行标题
    for col, header in enumerate(headers_row2, 1):
        worksheet.cell(row=2, column=col).value = header
    
    # 合并LTV相关的单元格（第一行）
    # 1日LTV (列12-13)
    worksheet.merge_cells(start_row=1, start_column=12, end_row=1, end_column=13)
    # 2日LTV (列14-15)
    worksheet.merge_cells(start_row=1, start_column=14, end_row=1, end_column=15)
    # 3日LTV (列16-17)
    worksheet.merge_cells(start_row=1, start_column=16, end_row=1, end_column=17)
    # 7日LTV (列18-19)
    worksheet.merge_cells(start_row=1, start_column=18, end_row=1, end_column=19)
    # 14日LTV (列20-21)
    worksheet.merge_cells(start_row=1, start_column=20, end_row=1, end_column=21)
    # 30日LTV (列22-23)
    worksheet.merge_cells(start_row=1, start_column=22, end_row=1, end_column=23)
    # 60日LTV (列24-25)
    worksheet.merge_cells(start_row=1, start_column=24, end_row=1, end_column=25)
    
    # 写入数据
    # 源文件列名说明：
    # - 基础列：时间, 消耗, 新增用户, 新增用户单价, 首充人数, 付费率, 注册复登, 首充复登, 3日注册复登, 3日首充复登, 首充成本
    # - LTV标签：首日LTV, 2日LTV, 3日LTV, 7日LTV, 14日LTV, 30日LTV, 60日LTV（这些只是标签，不含数据）
    # - LTV数据：金额, 付费用户数, 金额.1, 付费用户数.1, ... (pandas会自动加后缀)
    
    for row_idx, (_, row_data) in enumerate(df.iterrows(), 3):
        # 基础列（前11列）
        worksheet.cell(row=row_idx, column=1).value = row_data.get('时间', '')
        worksheet.cell(row=row_idx, column=2).value = row_data.get('消耗', '')
        worksheet.cell(row=row_idx, column=3).value = row_data.get('新增用户', '')
        worksheet.cell(row=row_idx, column=4).value = row_data.get('新增用户单价', '')
        worksheet.cell(row=row_idx, column=5).value = row_data.get('首充人数', '')
        worksheet.cell(row=row_idx, column=6).value = row_data.get('付费率', '')
        worksheet.cell(row=row_idx, column=7).value = row_data.get('注册复登', '')
        worksheet.cell(row=row_idx, column=8).value = row_data.get('首充复登', '')
        worksheet.cell(row=row_idx, column=9).value = row_data.get('3日注册复登', '')
        worksheet.cell(row=row_idx, column=10).value = row_data.get('3日首充复登', '')
        worksheet.cell(row=row_idx, column=11).value = row_data.get('首充成本', '')
        
        # 1日LTV数据 - 金额, 付费用户数（pandas不会给第一个加后缀）
        worksheet.cell(row=row_idx, column=12).value = row_data.get('金额', '')
        worksheet.cell(row=row_idx, column=13).value = row_data.get('付费用户数', '')
        
        # 2日LTV数据 - 金额.1, 付费用户数.1
        worksheet.cell(row=row_idx, column=14).value = row_data.get('金额.1', '')
        worksheet.cell(row=row_idx, column=15).value = row_data.get('付费用户数.1', '')
        
        # 3日LTV数据 - 金额.2, 付费用户数.2
        worksheet.cell(row=row_idx, column=16).value = row_data.get('金额.2', '')
        worksheet.cell(row=row_idx, column=17).value = row_data.get('付费用户数.2', '')
        
        # 7日LTV数据 - 金额.3, 付费用户数.3（跳过空列）
        worksheet.cell(row=row_idx, column=18).value = row_data.get('金额.3', '')
        worksheet.cell(row=row_idx, column=19).value = row_data.get('付费用户数.3', '')
        
        # 14日LTV数据 - 金额.4, 付费用户数.4
        worksheet.cell(row=row_idx, column=20).value = row_data.get('金额.4', '')
        worksheet.cell(row=row_idx, column=21).value = row_data.get('付费用户数.4', '')
        
        # 30日LTV数据 - 金额.5, 付费用户数.5
        worksheet.cell(row=row_idx, column=22).value = row_data.get('金额.5', '')
        worksheet.cell(row=row_idx, column=23).value = row_data.get('付费用户数.5', '')
        
        # 60日LTV数据 - 金额.6, 付费用户数.6
        worksheet.cell(row=row_idx, column=24).value = row_data.get('金额.6', '')
        worksheet.cell(row=row_idx, column=25).value = row_data.get('付费用户数.6', '')

def format_consumption_sheet(writer, df, rules):
    workbook = writer.book
    worksheet = workbook.create_sheet(title='消耗')
    
    # 第1行：总计行（各列的总和）
    worksheet.cell(row=1, column=2).value = '和'
    worksheet.cell(row=1, column=3).value = '日期'
    worksheet.cell(row=1, column=4).value = '当日消耗($)'
    
    # 第2行：列标题行
    worksheet.cell(row=2, column=1).value = '总代号匹配'
    worksheet.cell(row=2, column=2).value = '平台'
    worksheet.cell(row=2, column=3).value = '日期'
    worksheet.cell(row=2, column=4).value = '当日消耗($)'
    worksheet.cell(row=2, column=5).value = '短信'
    worksheet.cell(row=2, column=6).value = '短信'
    worksheet.cell(row=2, column=7).value = '投放'
    worksheet.cell(row=2, column=8).value = '投放'
    worksheet.cell(row=2, column=9).value = '投放'
    worksheet.cell(row=2, column=10).value = '投放'
    worksheet.cell(row=2, column=11).value = '网红'
    worksheet.cell(row=2, column=12).value = '渠道消耗'
    worksheet.cell(row=2, column=13).value = '核对'
    worksheet.cell(row=2, column=14).value = ''  # 无标题列

    # 隐藏第一列（总代号匹配）
    try:
        worksheet.column_dimensions[get_column_letter(1)].hidden = True
    except Exception:
        pass

    # 第3行及以下：每日数据
    start_row = 3
    end_row = start_row + len(df) - 1
    
    for r_idx, (date_index, row_data) in enumerate(df.iterrows(), start_row):
        worksheet.cell(row=r_idx, column=2).value = CONFIG["PLATFORM_NAME"]
        
        # 处理日期字段（透视表的索引是日期）
        date_val = date_index if isinstance(date_index, str) else str(date_index)
        worksheet.cell(row=r_idx, column=3).value = date_val
        
        # 处理各分类的消耗数据
        sms_consumption = row_data.get('短信', 0) if '短信' in row_data else 0
        ad_consumption = row_data.get('投放', 0) if '投放' in row_data else 0
        influencer_consumption = row_data.get('网红', 0) if '网红' in row_data else 0
        total_consumption = row_data.get('总计', 0) if '总计' in row_data else 0
        
        # 设置各分类的消耗数据（每类只占一列）
        worksheet.cell(row=r_idx, column=4).value = total_consumption  # 当日消耗($)
        worksheet.cell(row=r_idx, column=5).value = sms_consumption   # 短信（第1列）
        worksheet.cell(row=r_idx, column=6).value = ad_consumption    # 投放（第1列）
        worksheet.cell(row=r_idx, column=7).value = influencer_consumption  # 网红（第1列）
        
        # 渠道消耗 = 各渠道消耗的总和
        channel_total = sms_consumption + ad_consumption + influencer_consumption
        worksheet.cell(row=r_idx, column=12).value = channel_total
        
        # 核对 = 当日消耗($) 是否等于 渠道消耗
        worksheet.cell(row=r_idx, column=13).value = f"=IF(ROUND(D{r_idx},2)=ROUND(L{r_idx},2),\"TRUE\",\"FALSE\")"
        
        # 差值列 = 当日消耗($) - 渠道消耗
        worksheet.cell(row=r_idx, column=14).value = f"=D{r_idx}-L{r_idx}"
    
    # 第1行的总计公式（各列的总和）
    if start_row <= end_row:
        for col_idx in range(4, 15):  # D列到N列
            col_letter = get_column_letter(col_idx)
            worksheet.cell(row=1, column=col_idx).value = f"=SUM({col_letter}{start_row}:{col_letter}{end_row})"

def generate_daily_channel_sheet(q_files, u_files, e_files):
    if not any([q_files, u_files, e_files]):
        print(f"  Daily channel statistics files missing: q={len(q_files)}, u={len(u_files)}, e={len(e_files)}")
        return None
        
    print(f"  Processing daily channel statistics, file count: q={len(q_files)}, u={len(u_files)}, e={len(e_files)}")
        
    file_groups = {}
    all_files = q_files + u_files + e_files
    
    # 从文件名提取日期的函数
    def extract_date_from_filename(filename):
        """从文件名中提取日期，支持多种格式"""
        import re
        # 支持格式：-10-17, -10.17, 2025-10-17, 2025-10-18等
        patterns = [
            r'(\d{4})-(\d{1,2})-(\d{1,2})',  # 2025-10-17
            r'-(\d{1,2})\.(\d{1,2})',        # -10.17 (用点号)
            r'-(\d{1,2})-(\d{1,2})',         # -10-17 (用连字符)
        ]
        
        for pattern in patterns:
            match = re.search(pattern, filename)
            if match:
                if len(match.groups()) == 3:  # 2025-10-17格式
                    year, month, day = match.groups()
                    return f"{day}/{month}/{year}"
                elif len(match.groups()) == 2:  # -10-17 或 -10.17格式
                    month, day = match.groups()
                    current_year = datetime.now().year
                    return f"{day}/{month}/{current_year}"
        return None
    
    # 按日期分组，而不是按文件名分组
    for f in all_files:
        print(f"    Processing file: {os.path.basename(f).encode('ascii', 'ignore').decode('ascii')}")
        
        # 从文件名提取日期
        file_date = extract_date_from_filename(f)
        print(f"      Extracted date: {file_date}")
        
        # 使用日期作为key，而不是文件名
        key = file_date if file_date else "未知日期"
        
        if key not in file_groups:
            file_groups[key] = {}
            file_groups[key]['date'] = file_date  # 保存日期信息
            
        if '渠道质量' in f:
            file_groups[key]['quality'] = f
            print(f"      Categorized as: Channel Quality")
        elif '用户质量' in f:
            file_groups[key]['user'] = f
            print(f"      Categorized as: User Quality")
        elif '经济效益' in f:
            file_groups[key]['economic'] = f
            print(f"      Categorized as: Economic Benefit")

    all_days_df = []
    
    # 简化处理：直接处理所有文件，不要求三个类型都齐全
    for key in sorted(file_groups.keys()):
        group = file_groups[key]
        print(f"    Processing file group: {key}")
        print(f"      Contains files: {list(group.keys())}")
        
        # 只要有任何一个文件就处理
        if not group:
            continue

        try:
            # 处理可用的文件
            dfs = []
            if 'quality' in group:
                df_q = pd.read_csv(group['quality'])
                print(f"      Channel quality file: {df_q.shape}")
                dfs.append(df_q)
            if 'user' in group:
                df_u = pd.read_csv(group['user'])
                print(f"      User quality file: {df_u.shape}")
                dfs.append(df_u)
            if 'economic' in group:
                df_e = pd.read_csv(group['economic'])
                print(f"      Economic benefit file: {df_e.shape}")
                dfs.append(df_e)
            
            if not dfs:
                continue
                
            # 合并所有可用的数据
            merged_df = dfs[0]
            for df in dfs[1:]:
                if '分组名称' in df.columns and '分组名称' in merged_df.columns:
                    merged_df = pd.merge(merged_df, df, on='分组名称', how='outer')
                else:
                    merged_df = pd.concat([merged_df, df], ignore_index=True)
            
            # 基础清洗：去首尾空格
            merged_df.columns = merged_df.columns.map(lambda x: str(x).strip())
            
            # 统一常见别名，避免列名差异导致缺值
            rename_map = {
                    '老用户登录': '老用户登陆',
                    '访问': '访问量',
                    '新增': '新增用户',
                    'ARPU值': 'ARPU',
                    '当日消耗': '当日消耗($)',
                    '当日消耗(USD)': '当日消耗($)',
                    '当天消耗': '当日消耗($)',
                }
            for k, v in list(rename_map.items()):
                if k in merged_df.columns and v not in merged_df.columns:
                    merged_df.rename(columns={k: v}, inplace=True)
            
            # 使用从文件名提取的日期
            report_date = group.get('date')
            if not report_date:
                # 如果文件名中没有日期，尝试从数据中获取
                if '日期' in merged_df.columns and not merged_df['日期'].dropna().empty:
                    first_valid_date = merged_df['日期'].dropna().iloc[0]
                    parsed = pd.to_datetime(first_valid_date, errors='coerce')
                    if pd.notna(parsed):
                        report_date = parsed.strftime('%#d/%#m/%Y')
                else:
                    # 如果都没有，使用当前日期
                    report_date = datetime.now().strftime('%#d/%#m/%Y')
            
            print(f"      Using date: {report_date}")

            # 过滤掉汇总行
            if '分组名称' in merged_df.columns:
                merged_df = merged_df[~merged_df['分组名称'].astype(str).str.contains('数据汇总|总计', na=False, regex=True)].copy()
            
            merged_df['日期'] = report_date if report_date else "未知日期"
            all_days_df.append(merged_df)
            print(f"      Merged data: {merged_df.shape}")

        except Exception as e:
            print(f"Error processing file group {key}: {e}")
            continue
            
    if not all_days_df:
        return None
        
    final_df = pd.concat(all_days_df, ignore_index=True)

    def parse_group_info(group_name):
        code_to_category_map = {rule['代码']: rule['分类'] for rule in CHANNEL_RULES}
        code, name, category, delivery_method = '', '', '', ''
        try:
            parts = str(group_name).strip().split(' / ')
            if len(parts) >= 2:
                code, name = parts[0].strip(), parts[1].strip()
                category = code_to_category_map.get(code, '其他')
                if 'ORGANIC' in name.upper():
                    category = '自然量'
                    delivery_method = 'ORGANIC'
                elif '_' in name:
                    delivery_method = name.split('_')[-1]
            elif len(parts) == 1:
                name = parts[0].strip()
                if 'ORGANIC' in name.upper():
                    category = '自然量'
                    delivery_method = 'ORGANIC'
        except Exception: pass
        return code, name, category, delivery_method

    parsed_info = final_df['分组名称'].apply(lambda x: pd.Series(parse_group_info(x), index=['总代号', '总代名', '方式', '投放方式']))
    final_df = pd.concat([final_df, parsed_info], axis=1)

    def parse_composite_column(series, part_index):
        if series.empty or series.dropna().empty: return pd.Series([0] * len(series), index=series.index)
        temp_df = series.astype(str).str.split(r'\s*\|\s*', expand=True)
        if part_index < len(temp_df.columns):
            return pd.to_numeric(temp_df[part_index], errors='coerce').fillna(0)
        return pd.Series([0] * len(series), index=series.index)

    final_df['一级首充人数'] = parse_composite_column(final_df.get('一级首充'), 1)
    final_df['首充金额'] = parse_composite_column(final_df.get('首充'), 0)
    final_df['首充人数'] = parse_composite_column(final_df.get('首充'), 1)
    final_df['非首充充值金额'] = parse_composite_column(final_df.get('非首充'), 0)
    final_df['非首充人数'] = parse_composite_column(final_df.get('非首充'), 1)
    
    final_df.rename(columns={'首充': '首充（首充人均）','非首充': '非首充（老用户人均）','新增充值': '新增充值（充值人均)','总收入': '充值金額'}, inplace=True)
    
    output_columns = ['日期', '总代号', '总代名', '方式', '投放方式', '一级首充人数', '首充金额', '首充人数', '非首充充值金额', '非首充人数', '分组名称', '当日消耗($)', '访问量', '新增用户', '老用户登陆', '活跃', '新增成本', '首充付费人数', '付费/首充成本', '注册复登', '首充复登', '首充复充', '首充（首充人均）', '一级首充', '非首充（老用户人均）', '新增充值（充值人均)', '一级新充', '裂变占比', '新增/首充率', '老用户充值', '活跃充值', '提现人数', '提现金额', '提现率', '首充提现率(人数)', '首充提现率(金额)', '老用户付费率', '老用户付费占比', '活跃付费率', 'ARPU', '充值金額', '总提现', '充提差', '新增充值ROAS', '老用户充值ROAS', '总充值ROI', '当日回本ROI']
    
    for col in output_columns:
        if col not in final_df.columns:
            final_df[col] = ''
            
    return final_df[output_columns]

def generate_daily_summary_report(backend_files, channel_quality_files, user_quality_files, economic_files):
    """
    生成VNL大盘日报，包含每日详细数据和总计、日均数据
    """
    print("正在生成VNL大盘日报...")
    
    # 处理后台数据
    backend_data = None
    if backend_files:
        try:
            backend_df = process_cumulative_files(backend_files, '时间', sort_by_col=None)
            if backend_df is not None and not backend_df.empty:
                backend_data = backend_df.sort_values(by='时间').reset_index(drop=True)
                print(f"  后台数据: {len(backend_data)} 行")
        except Exception as e:
            print(f"  处理后台数据出错: {e}")
    
    # 处理渠道数据
    channel_data = None
    if all([channel_quality_files, user_quality_files, economic_files]):
        try:
            df_q = pd.read_csv(channel_quality_files[0], sep=',', engine='python', on_bad_lines='skip', encoding='utf-8')
            df_u = pd.read_csv(user_quality_files[0], sep=',', engine='python', on_bad_lines='skip', encoding='utf-8')
            df_e = pd.read_csv(economic_files[0], sep=',', engine='python', on_bad_lines='skip', encoding='utf-8')
            
            merged = pd.merge(df_q, df_u, on='日期', how='outer', suffixes=('_q', '_u'))
            merged = pd.merge(merged, df_e, on='日期', how='outer')
            
            # 清洗数据
            merged = merged[~merged['日期'].astype(str).str.contains('数据汇总|总计', na=False, regex=True)].copy()
            merged['日期'] = pd.to_datetime(merged['日期'], errors='coerce')
            merged.dropna(subset=['日期'], inplace=True)
            
            if not merged.empty:
                channel_data = merged.sort_values(by='日期').reset_index(drop=True)
                print(f"  渠道数据: {len(channel_data)} 行")
        except Exception as e:
            print(f"  处理渠道数据出错: {e}")
    
    # 合并数据生成大盘日报
    if backend_data is None and channel_data is None:
        print("  警告：没有可用的数据生成大盘日报")
        return None
    
    # 创建大盘日报数据结构
    daily_report_data = []
    
    # 获取日期范围
    all_dates = set()
    if backend_data is not None:
        all_dates.update(backend_data['时间'].dt.date)
    if channel_data is not None:
        all_dates.update(channel_data['日期'].dt.date)
    
    if not all_dates:
        print("  警告：没有找到有效日期")
        return None
    
    # 按日期排序
    sorted_dates = sorted(all_dates)
    
    for date in sorted_dates:
        # 查找对应日期的数据
        backend_row = None
        channel_row = None
        
        if backend_data is not None:
            backend_match = backend_data[backend_data['时间'].dt.date == date]
            if not backend_match.empty:
                backend_row = backend_match.iloc[0]
        
        if channel_data is not None:
            channel_match = channel_data[channel_data['日期'].dt.date == date]
            if not channel_match.empty:
                channel_row = channel_match.iloc[0]
        
        # 构建日报行数据
        row_data = {
            '日期': date.strftime('%#d/%#m/%Y'),
            '总消耗(U)': backend_row.get('当日总流水', 0) if backend_row is not None else 0,
            '首充成本(U)': 0,  # 需要计算
            '一级首充成本(U)': 0,  # 需要计算
            '充提差比': 0,  # 需要计算
            '总充提差(U)': backend_row.get('充提差', 0) if backend_row is not None else 0,
            '充值金额(U)': backend_row.get('商城充值金额', 0) if backend_row is not None else 0,
            '首充充值金额(U)': 0,  # 需要计算
            '老用户充值金额(U)': 0,  # 需要计算
            '人均充值金额(U)': backend_row.get('人均充值', 0) if backend_row is not None else 0,
            '人均充值笔数': 0,  # 需要计算
            '首充人均充值金额(U)': 0,  # 需要计算
            '老用户人均充值金额U': 0,  # 需要计算
            '老用户付费率': 0,  # 需要计算
            '14日LTV': 0,  # 需要从LTV数据获取
            '30日LTV': 0,  # 需要从LTV数据获取
            '首充次日复充率': 0,  # 需要计算
            '首充3日复充率': 0,  # 需要计算
            '首充7日复充率': 0,  # 需要计算
            '首充30日复充率': 0,  # 需要计算
            '非一级首充人数/首充人数': 0,  # 需要计算
            '非一级首充人数/充值人数': 0,  # 需要计算
            '首充数转化率': 0,  # 需要计算
            '新增注册人数': backend_row.get('新增人员', 0) if backend_row is not None else 0,
            '首充人数': backend_row.get('新增充值人数', 0) if backend_row is not None else 0,
            '一级首充人数': backend_row.get('一级新充人数', 0) if backend_row is not None else 0,
            '充值人数': backend_row.get('充值人数', 0) if backend_row is not None else 0,
            '投注用户数': backend_row.get('投注人数', 0) if backend_row is not None else 0,
            '充投用户数': backend_row.get('充值投注人数', 0) if backend_row is not None else 0,
            '首充次日复登率': 0,  # 需要计算
            '首充3日复登率': 0,  # 需要计算
            '首充7日复登率': 0,  # 需要计算
            '首充次日复投率': 0,  # 需要计算
            '首充3日复投率': 0,  # 需要计算
            '首充7日复投率': 0,  # 需要计算
            '投注金额(U)': 0,  # 需要计算
            '出入人数比': 0,  # 需要计算
            '游戏盈亏投注比': 0,  # 需要计算
            '投充比': backend_row.get('今日投充比', 0) if backend_row is not None else 0,
            '佣金金额(U)': 0,  # 需要计算
            '彩金赠送充值比': 0,  # 需要计算
            '历史营收(历史充提差-历史消耗)': 0,  # 需要计算
            '历史充提差': 0,  # 需要计算
            '历史消耗': 0,  # 需要计算
        }
        
        daily_report_data.append(row_data)
    
    # 创建DataFrame
    daily_df = pd.DataFrame(daily_report_data)
    
    # 计算总计和日均
    total_row = {
        '日期': '总计',
        '总消耗(U)': daily_df['总消耗(U)'].sum(),
        '首充成本(U)': 0,  # 需要计算
        '一级首充成本(U)': 0,  # 需要计算
        '充提差比': 0,  # 需要计算
        '总充提差(U)': daily_df['总充提差(U)'].sum(),
        '充值金额(U)': daily_df['充值金额(U)'].sum(),
        '首充充值金额(U)': 0,  # 需要计算
        '老用户充值金额(U)': 0,  # 需要计算
        '人均充值金额(U)': 0,  # 需要计算
        '人均充值笔数': 0,  # 需要计算
        '首充人均充值金额(U)': 0,  # 需要计算
        '老用户人均充值金额U': 0,  # 需要计算
        '老用户付费率': 0,  # 需要计算
        '14日LTV': 0,  # 需要计算
        '30日LTV': 0,  # 需要计算
        '首充次日复充率': 0,  # 需要计算
        '首充3日复充率': 0,  # 需要计算
        '首充7日复充率': 0,  # 需要计算
        '首充30日复充率': 0,  # 需要计算
        '非一级首充人数/首充人数': 0,  # 需要计算
        '非一级首充人数/充值人数': 0,  # 需要计算
        '首充数转化率': 0,  # 需要计算
        '新增注册人数': daily_df['新增注册人数'].sum(),
        '首充人数': daily_df['首充人数'].sum(),
        '一级首充人数': daily_df['一级首充人数'].sum(),
        '充值人数': daily_df['充值人数'].sum(),
        '投注用户数': daily_df['投注用户数'].sum(),
        '充投用户数': daily_df['充投用户数'].sum(),
        '首充次日复登率': 0,  # 需要计算
        '首充3日复登率': 0,  # 需要计算
        '首充7日复登率': 0,  # 需要计算
        '首充次日复投率': 0,  # 需要计算
        '首充3日复投率': 0,  # 需要计算
        '首充7日复投率': 0,  # 需要计算
        '投注金额(U)': 0,  # 需要计算
        '出入人数比': 0,  # 需要计算
        '游戏盈亏投注比': 0,  # 需要计算
        '投充比': 0,  # 需要计算
        '佣金金额(U)': 0,  # 需要计算
        '彩金赠送充值比': 0,  # 需要计算
        '历史营收(历史充提差-历史消耗)': 0,  # 需要计算
        '历史充提差': 0,  # 需要计算
        '历史消耗': 0,  # 需要计算
    }
    
    # 计算日均
    days_count = len(daily_df)
    avg_row = {
        '日期': '日均',
        '总消耗(U)': daily_df['总消耗(U)'].mean(),
        '首充成本(U)': 0,  # 需要计算
        '一级首充成本(U)': 0,  # 需要计算
        '充提差比': 0,  # 需要计算
        '总充提差(U)': daily_df['总充提差(U)'].mean(),
        '充值金额(U)': daily_df['充值金额(U)'].mean(),
        '首充充值金额(U)': 0,  # 需要计算
        '老用户充值金额(U)': 0,  # 需要计算
        '人均充值金额(U)': 0,  # 需要计算
        '人均充值笔数': 0,  # 需要计算
        '首充人均充值金额(U)': 0,  # 需要计算
        '老用户人均充值金额U': 0,  # 需要计算
        '老用户付费率': 0,  # 需要计算
        '14日LTV': 0,  # 需要计算
        '30日LTV': 0,  # 需要计算
        '首充次日复充率': 0,  # 需要计算
        '首充3日复充率': 0,  # 需要计算
        '首充7日复充率': 0,  # 需要计算
        '首充30日复充率': 0,  # 需要计算
        '非一级首充人数/首充人数': 0,  # 需要计算
        '非一级首充人数/充值人数': 0,  # 需要计算
        '首充数转化率': 0,  # 需要计算
        '新增注册人数': daily_df['新增注册人数'].mean(),
        '首充人数': daily_df['首充人数'].mean(),
        '一级首充人数': daily_df['一级首充人数'].mean(),
        '充值人数': daily_df['充值人数'].mean(),
        '投注用户数': daily_df['投注用户数'].mean(),
        '充投用户数': daily_df['充投用户数'].mean(),
        '首充次日复登率': 0,  # 需要计算
        '首充3日复登率': 0,  # 需要计算
        '首充7日复登率': 0,  # 需要计算
        '首充次日复投率': 0,  # 需要计算
        '首充3日复投率': 0,  # 需要计算
        '首充7日复投率': 0,  # 需要计算
        '投注金额(U)': 0,  # 需要计算
        '出入人数比': 0,  # 需要计算
        '游戏盈亏投注比': 0,  # 需要计算
        '投充比': 0,  # 需要计算
        '佣金金额(U)': 0,  # 需要计算
        '彩金赠送充值比': 0,  # 需要计算
        '历史营收(历史充提差-历史消耗)': 0,  # 需要计算
        '历史充提差': 0,  # 需要计算
        '历史消耗': 0,  # 需要计算
    }
    
    # 合并所有数据
    final_df = pd.concat([daily_df, pd.DataFrame([total_row]), pd.DataFrame([avg_row])], ignore_index=True)
    
    print(f"  大盘日报数据: {len(final_df)} 行（包含总计和日均）")
    return final_df

def format_daily_summary_sheet(writer, df, sheet_name='VNL大盘日报'):
    """
    格式化VNL大盘日报sheet，包含单元格内柱状图等可视化元素
    """
    workbook = writer.book
    worksheet = workbook.create_sheet(title=sheet_name)
    
    # 设置表头
    headers = [
        '日期', '总消耗(U)', '首充成本(U)', '一级首充成本(U)', '充提差比', '总充提差(U)', 
        '充值金额(U)', '首充充值金额(U)', '老用户充值金额(U)', '人均充值金额(U)', 
        '人均充值笔数', '首充人均充值金额(U)', '老用户人均充值金额U', '老用户付费率', 
        '14日LTV', '30日LTV', '首充次日复充率', '首充3日复充率', '首充7日复充率', 
        '首充30日复充率', '非一级首充人数/首充人数', '非一级首充人数/充值人数', 
        '首充数转化率', '新增注册人数', '首充人数', '一级首充人数', '充值人数',
        '投注用户数', '充投用户数', '首充次日复登率', '首充3日复登率', '首充7日复登率',
        '首充次日复投率', '首充3日复投率', '首充7日复投率', '投注金额(U)', 
        '出入人数比', '游戏盈亏投注比', '投充比', '佣金金额(U)', '彩金赠送充值比',
        '历史营收(历史充提差-历史消耗)', '历史充提差', '历史消耗'
    ]
    
    # 写入表头
    for col_idx, header in enumerate(headers, 1):
        worksheet.cell(row=1, column=col_idx).value = header
    
    # 写入数据
    for row_idx, (_, row) in enumerate(df.iterrows(), 2):
        for col_idx, header in enumerate(headers, 1):
            value = row.get(header, '')
            if isinstance(value, (int, float)) and value != 0:
                worksheet.cell(row=row_idx, column=col_idx).value = value
            else:
                worksheet.cell(row=row_idx, column=col_idx).value = value
    
    # 添加单元格内柱状图效果（条件格式）
    try:
        # 获取数据行数（排除总计和日均行）
        data_rows = len(df) - 2
        if data_rows > 0:
            # 为复充率列添加数据条效果
            retention_columns = [
                ('首充次日复充率', 17),  # Q列
                ('首充3日复充率', 18),   # R列
                ('首充7日复充率', 19),   # S列
                ('首充30日复充率', 20),  # T列
            ]
            
            for col_name, col_idx in retention_columns:
                # 创建数据条规则
                data_bar_rule = DataBarRule(
                    start_type='min',
                    start_value=0,
                    end_type='max',
                    end_value=100,
                    color='4F81BD',  # 蓝色
                    showValue=True,
                    minLength=None,
                    maxLength=None
                )
                
                # 应用数据条规则到数据区域（排除总计和日均行）
                data_range = f"{get_column_letter(col_idx)}2:{get_column_letter(col_idx)}{data_rows + 1}"
                worksheet.conditional_formatting.add(data_range, data_bar_rule)
            
            # 为其他数值列添加数据条效果
            numeric_columns = [
                ('总消耗(U)', 2),      # B列
                ('充值金额(U)', 7),     # G列
                ('新增注册人数', 24),   # X列
                ('首充人数', 25),       # Y列
                ('充值人数', 28),       # AB列
            ]
            
            for col_name, col_idx in numeric_columns:
                # 创建数据条规则
                data_bar_rule = DataBarRule(
                    start_type='min',
                    start_value=0,
                    end_type='max',
                    end_value=None,
                    color='70AD47',  # 绿色
                    showValue=True,
                    minLength=None,
                    maxLength=None
                )
                
                # 应用数据条规则到数据区域
                data_range = f"{get_column_letter(col_idx)}2:{get_column_letter(col_idx)}{data_rows + 1}"
                worksheet.conditional_formatting.add(data_range, data_bar_rule)
            
            print(f"  已添加单元格内柱状图效果到 {sheet_name}")
    except Exception as e:
        print(f"  添加条件格式时出错: {e}")
    
    # 设置列宽
    for col_idx in range(1, len(headers) + 1):
        column_letter = get_column_letter(col_idx)
        worksheet.column_dimensions[column_letter].width = 15
    
    print(f"  {sheet_name} sheet 已格式化完成")


def main():
    script_dir = os.path.dirname(os.path.abspath(__file__))
    output_excel_filename = os.path.join(script_dir, CONFIG["OUTPUT_FILENAME"])
    
    # 检查是否存在现有的Excel文件
    existing_data = {}
    is_incremental = False
    
    if os.path.exists(output_excel_filename):
        print(f"Found existing Excel file: {os.path.basename(output_excel_filename)}")
        print("Will use incremental update mode (preserve historical data, update/add new data)...")
        is_incremental = True
        
        # 读取现有数据，用于智能更新
        try:
            existing_df = pd.read_excel(output_excel_filename, sheet_name=None)
            for sheet_name, df in existing_df.items():
                if '时间' in df.columns:
                    # 将时间转换为字符串用于比较
                    existing_data[sheet_name] = set(df['时间'].astype(str).tolist())
            print(f"Existing data contains {len(existing_data)} sheets")
        except Exception as e:
            print(f"Error reading existing file: {e}")
            print("Will use complete rewrite mode...")
            is_incremental = False
    else:
        print("No existing Excel file found, will create new file...")
    
    def find_files(pattern):
        files = glob.glob(os.path.join(script_dir, pattern))
        print(f"Found {len(files)} files for pattern.")
        return files

    backend_files = find_files("download_ops_*.csv")
    channel_quality_files = find_files("*推广渠道统计-渠道质量*.csv")
    user_quality_files = find_files("*推广渠道统计-用户质量*.csv")
    economic_files = find_files("*推广渠道统计-经济效益*.csv")
    
    # 如果没找到带月份的文件，尝试查找其他格式
    if not channel_quality_files:
        channel_quality_files = find_files("*渠道质量*.csv")
    if not user_quality_files:
        user_quality_files = find_files("*用户质量*.csv")
    if not economic_files:
        economic_files = find_files("*经济效益*.csv")
    
    # 查找全平台留存文件
    login_return_files = find_files("用户留存率_首充复登_*全平台*.csv")
    if not login_return_files:
        login_return_files = find_files("download_first_login_return_*全平台*.csv")
    if not login_return_files:
        login_return_files = find_files("*首充复登*全平台*.csv")
    
    recharge_return_files = find_files("用户留存率_首充复充_*全平台*.csv")
    if not recharge_return_files:
        recharge_return_files = find_files("download_first_recharge_return_*全平台*.csv")
    if not recharge_return_files:
        recharge_return_files = find_files("*首充复充*全平台*.csv")
    
    print(f"Retention rate related files:")
    print(f"  - Login retention files: {len(login_return_files)}")
    print(f"  - Recharge retention files: {len(recharge_return_files)}")

    # 只保留包含留存列的文件
    def _file_has_retention_cols(fp):
        try:
            tmp = pd.read_csv(fp, nrows=1, engine='python', on_bad_lines='skip', encoding='utf-8')
            tmp.columns = tmp.columns.str.strip()
            # 临时复制规范化逻辑检查是否存在任一留存列
            cols = set(tmp.columns)
            if '时间' not in cols and '日期' not in cols:
                return False
            # 直接看是否有包含“日”或常见 Dn/次日 的列
            for c in cols:
                cs = str(c)
                if cs == '次日' or ('日' in cs and any(ch.isdigit() for ch in cs)) or re.match(r'^[Dd](\d+)', cs):
                    return True
            return False
        except Exception:
            return False

    login_return_files = [f for f in login_return_files if _file_has_retention_cols(f)]
    recharge_return_files = [f for f in recharge_return_files if _file_has_retention_cols(f)]
    print(f"  - Filtered login retention files: {len(login_return_files)}")
    print(f"  - Filtered recharge retention files: {len(recharge_return_files)}")
    
    all_promo_files = find_files("推广_*.csv")
    consumption_files = [f for f in all_promo_files if '渠道质量' in os.path.basename(f)]
    print(f"  - Filtered consumption files: {len(consumption_files)}")
    
    daily_q_files = find_files("*每日渠道统计-渠道质量*.csv")
    daily_u_files = find_files("*每日渠道统计-用户质量*.csv")
    daily_e_files = find_files("*每日渠道统计-经济效益*.csv")
    
    # 赠送类型统计文件
    gift_stats_files = find_files("*赠送类型统计*.csv")

    # 渠道留存文件（按总代号分组）
    # 渠道首充复登使用 用户留存率_首充复登 文件
    channel_login_files = find_files("用户留存率_首充复登_*_*.csv")
    if not channel_login_files:
    channel_login_files = find_files("download_first_recharge_return_*_*.csv")
    
    # 渠道首充复充使用 用户留存率_首充复充 文件
    channel_recharge_files = find_files("用户留存率_首充复充_*_*.csv")
    
    # 合并中英文文件
    all_channel_recharge_files = channel_recharge_files
    
    print(f"Channel retention files:")
    print(f"  - Channel login retention (首充复登): {len(channel_login_files)}")
    print(f"  - Channel recharge retention (首充复充): {len(channel_recharge_files)}")
    print(f"  - Total channel recharge files: {len(all_channel_recharge_files)}")

    print("\n--- Starting to write Excel file ---")
    
    # 根据是否为增量模式决定是否删除现有文件
    if not is_incremental:
        if os.path.exists(output_excel_filename):
            try:
                os.remove(output_excel_filename)
            except PermissionError:
                print(f"\nError: Cannot delete old report file '{output_excel_filename}'.")
                print("Please close the Excel file manually and run the script again.")
                return
        pd.DataFrame().to_excel(output_excel_filename)
    else:
        # 增量模式：保留现有文件，只更新数据
        print("Incremental update mode: will preserve existing data and add new data")

    with pd.ExcelWriter(output_excel_filename, engine='openpyxl') as writer:
        
        print("\nProcessing 'Backend' sheet...")
        if (backend_data := generate_backend_sheet(backend_files, existing_data.get('后台', None), is_incremental)) is not None and not backend_data.empty:
            if is_incremental and '后台' in existing_data:
                # 增量模式：合并新旧数据
                try:
                    existing_backend = pd.read_excel(output_excel_filename, sheet_name='后台')
                    combined_data = pd.concat([existing_backend, backend_data], ignore_index=True)
                    combined_data = combined_data.drop_duplicates(subset=['时间'], keep='last').sort_values('时间')
                    combined_data.to_excel(writer, sheet_name='后台', index=False)
                    print("- 'Backend' sheet updated incrementally.")
                except Exception as e:
                    print(f"Incremental update failed, using new data: {e}")
                    backend_data.to_excel(writer, sheet_name='后台', index=False)
                    print("- 'Backend' sheet written.")
            else:
                backend_data.to_excel(writer, sheet_name='后台', index=False)
                print("- 'Backend' sheet written.")

        print("\nProcessing 'Promotion Channel Statistics' sheet...")
        if (channel_data := generate_channel_sheet(channel_quality_files, user_quality_files, economic_files)) is not None and not channel_data.empty:
            channel_data.to_excel(writer, sheet_name='推广渠道统计', index=False)
            print("- 'Promotion Channel Statistics' sheet written.")
        
        # 3. 首充复登
        print("\nProcessing 'First Pay Login Retention' sheet...")
        if (login_data := generate_retention_data(login_return_files)) is not None and not login_data.empty:
            summary_cols_login = ['次日', '3日', '4日', '5日', '6日', '7日']
            format_retention_sheet(writer, login_data, '首充复登', summary_cols_login)
            print("- 'First Pay Login Retention' sheet written and formatted.")
            
            # 保存首充人数数据，供首充复充使用
            first_pay_data = login_data[['时间', '首充人数']].copy() if '首充人数' in login_data.columns else None
        else:
            first_pay_data = None

        # 4. 首充复充
        print("\nProcessing 'First Pay Recharge Retention' sheet...")
        # 首充复充使用中文命名的文件
        recharge_recharge_files = find_files("用户留存率_首充复充_*全平台*.csv")
        if not recharge_recharge_files:
            recharge_recharge_files = find_files("*首充复充*全平台*.csv")
        
        if (recharge_data := generate_retention_data(recharge_recharge_files)) is not None and not recharge_data.empty:
            # 如果首充复登有首充人数数据，使用相同的数据
            if first_pay_data is not None and '首充人数' in recharge_data.columns:
                print("  Using first pay login retention data to ensure consistency")
                recharge_data = recharge_data.drop('首充人数', axis=1)
                recharge_data = recharge_data.merge(first_pay_data, on='时间', how='left')
            
            summary_cols_recharge = ['次日', '3日', '4日', '5日', '6日', '7日', '30日']
            format_retention_sheet(writer, recharge_data, '首充复充', summary_cols_recharge)
            print("- 'First Pay Recharge Retention' sheet written and formatted.")
            
        # 5. 赠送类型统计
        print("\n正在处理 '赠送类型统计' sheet...")
        if gift_stats_files:
            try:
                gift_data_list = []
                for file in gift_stats_files:
                    try:
                        df = pd.read_csv(file, encoding='utf-8')
                    except:
                        try:
                            df = pd.read_csv(file, encoding='gbk')
                        except:
                            df = pd.read_csv(file, encoding='latin-1')
                    
                    print(f"  - 加载 {os.path.basename(file)}: {df.shape}")
                    print(f"    列名: {list(df.columns)}")
                    gift_data_list.append(df)
                
                if gift_data_list:
                    # 合并数据
                    gift_combined = pd.concat(gift_data_list, ignore_index=True)
                    
                    # 创建标准化的赠送类型统计表
                    workbook = writer.book
                    ws = workbook.create_sheet(title='赠送类型统计')
                    
                    # 设置表头 - A列是日期列
                    headers = ['日期', '赠送类型', '赠送数量', '赠送金额', '备注']
                    for col_idx, header in enumerate(headers, 1):
                        ws.cell(row=1, column=col_idx).value = header
                    
                    # 写入数据
                    row = 2
                    for file in gift_stats_files:
                        # 从文件名中提取日期
                        filename = os.path.basename(file)
                        date_from_filename = None
                        
                        # 尝试从文件名中提取日期 (格式: 赠送类型统计2025-10-17.csv)
                        date_match = re.search(r'(\d{4}-\d{1,2}-\d{1,2})', filename)
                        if date_match:
                            date_from_filename = date_match.group(1)
                            print(f"  Extracted date from filename: {date_from_filename}")
                        
                        # 读取该文件的数据
                        try:
                            df = pd.read_csv(file, encoding='utf-8')
                        except:
                            try:
                                df = pd.read_csv(file, encoding='gbk')
                            except:
                                df = pd.read_csv(file, encoding='latin-1')
                        
                        # 为每一行数据写入日期
                        for _, data_row in df.iterrows():
                            # A列：日期（优先使用文件名中的日期）
                            date_val = date_from_filename or data_row.get('日期') or data_row.get('时间') or datetime.now().strftime('%Y-%m-%d')
                            ws.cell(row=row, column=1).value = date_val
                            
                            # 其他列：根据实际数据结构填充
                            for col_idx, header in enumerate(headers[1:], 2):
                                value = data_row.get(header) or data_row.get(header.replace('赠送', '')) or ''
                                ws.cell(row=row, column=col_idx).value = value
                            
                            row += 1
                    
                    print("- '赠送类型统计' sheet 已写入并格式化。")
            except Exception as e:
                print(f"  处理赠送类型统计时出错: {e}")
        else:
            print("  - 未找到赠送类型统计文件")
        
        # 6. LTV
        print("\n正在处理 'LTV' sheet...")
        # LTV相关文件
        ltv_files = find_files("download_ltv_*.csv")
        print(f"LTV文件:")
        print(f"  - 找到 {len(ltv_files)} 个LTV文件")
        
        # 按渠道分类LTV文件
        ltv_platform_files = [f for f in ltv_files if '全平台' in f]
        ltv_channel_files = [f for f in ltv_files if '全平台' not in f]
        
        # Build channel code to category mapping
        code_to_category = {}
        for rule in CHANNEL_RULES:
            code_to_category[rule['代码']] = rule['分类']
        
        # 按渠道类型分类（根据渠道代号直接分类）
        sms_ltv_files = []
        influencer_ltv_files = []
        ad_ltv_files = []
        
        for f in ltv_channel_files:
            basename = os.path.basename(f)
            # Extract channel code (e.g., 1102, 1103, 1104, etc.)
            match = re.search(r'download_ltv_(\d+)', basename)
            if match:
                channel_code = match.group(1)
                # Check channel code in CHANNEL_RULES to determine category
                category = code_to_category.get(channel_code)
                
                if category == '短信':
                    sms_ltv_files.append(f)
                    print(f"    SMS LTV file: {basename} (code: {channel_code})")
                elif category == '网红':
                    influencer_ltv_files.append(f)
                    print(f"    Influencer LTV file: {basename} (code: {channel_code})")
                elif category == '投放':
                    ad_ltv_files.append(f)
                    print(f"    Ad LTV file: {basename} (code: {channel_code})")
        
        print(f"  - Full platform LTV: {len(ltv_platform_files)} files")
        print(f"  - SMS LTV: {len(sms_ltv_files)} files")
        print(f"  - Influencer LTV: {len(influencer_ltv_files)} files")
        print(f"  - Ad LTV: {len(ad_ltv_files)} files")
        
        if ltv_platform_files:
            # 生成全平台LTV sheet
            try:
                # Only process files with current or previous month in filename
                filtered_ltv_platform_files = [f for f in ltv_platform_files if '本月' in f or '上月' in f]
                print(f"  - Filtered to {len(filtered_ltv_platform_files)} platform files (current and previous month only)")
                
                ltv_data_list = []
                for file in filtered_ltv_platform_files:
                    try:
                        df = pd.read_csv(file, encoding='utf-8')
                        ltv_data_list.append(df)
                    except:
                        try:
                            df = pd.read_csv(file, encoding='gbk')
                            ltv_data_list.append(df)
                        except:
                            df = pd.read_csv(file, encoding='latin-1')
                            ltv_data_list.append(df)
                
                if ltv_data_list:
                    ltv_combined = pd.concat(ltv_data_list, ignore_index=True)
                    
                    # Deduplicate by time, keep the last one (current month takes precedence)
                    if '时间' in ltv_combined.columns:
                        ltv_combined = ltv_combined.drop_duplicates(subset=['时间'], keep='last')
                        print(f"  - After deduplication: {len(ltv_combined)} rows")
                        ltv_combined = ltv_combined.sort_values('时间')
                    
                    # 使用专门的平台LTV格式化函数（简化表头，只有14/30/60日）
                    format_platform_ltv_sheet(writer, ltv_combined, 'LTV')
                    print("  - 'LTV' (Platform) sheet written with formatted headers.")
            except Exception as e:
                print(f"  Error processing LTV platform files: {e}")
                import traceback
                traceback.print_exc()
        
        # 7. 消耗
        print("\nProcessing 'Consumption' sheet...")
        # 合并所有LTV文件用于消耗数据提取，但只使用本月和上月的文件
        all_ltv_files = sms_ltv_files + influencer_ltv_files + ad_ltv_files
        filtered_consumption_files = [f for f in all_ltv_files if '本月' in f or '上月' in f]
        print(f"  - Using {len(filtered_consumption_files)} LTV files for consumption data (current and previous month only)")
        
        if (consumption_data := generate_consumption_data(filtered_consumption_files, CHANNEL_RULES)) is not None and not consumption_data.empty:
            format_consumption_sheet(writer, consumption_data, CHANNEL_RULES)
            print("- 'Consumption' sheet written and formatted.")
        else:
            print("  - No consumption data found")

        # 8. 每日渠道统计
        print("\nProcessing 'Daily Channel Statistics' sheet...")
        def format_daily_channel_sheet(writer, df):
            workbook = writer.book
            ws = workbook.create_sheet(title='每日渠道统计')
            headers = ['日期', '总代号', '总代名', '方式', '投放方式', '一级首充人数', '首充金额', '首充人数', '非首充充值金额', '非首充人数', '分组名称', '当日消耗($)', '访问量', '新增用户', '老用户登陆', '活跃', '新增成本', '首充付费人数', '付费/首充成本', '注册复登', '首充复登', '首充复充', '分组名称', '首充（首充人均）', '一级首充', '非首充（老用户人均）', '新增充值（充值人均)', '一级新充', '裂变占比', '新增/首充率', '老用户充值', '活跃充值', '提现人数', '提现金额', '提现率', '首充提现率(人数)', '首充提现率(金额)', '老用户付费率', '老用户付费占比', '活跃付费率', 'ARPU', '分组名称', '充值金額', '总提现', '充提差', '新增充值ROAS', '老用户充值ROAS', '总充值ROI', '当日回本ROI']
            for c_idx, h in enumerate(headers, 1):
                ws.cell(row=1, column=c_idx).value = h

            # 目标列索引（1-based）
            col_idx_map = {name: headers.index(name) + 1 for name in headers}

            # 获取引用列位置（后部原始列）
            col_group_name = col_idx_map['分组名称']  # 第一个分组名称位置
            col_first_charge = col_idx_map['首充（首充人均）']
            col_first_charge_lv1 = col_idx_map['一级首充']
            col_non_first = col_idx_map['非首充（老用户人均）']

            start_row = 2
            for r_offset, (_, row) in enumerate(df.iterrows(), 0):
                r = start_row + r_offset
                # 日期：优先使用数据中的日期，如果缺失则使用从文件名提取的日期
                date_val = row.get('日期')
                if not isinstance(date_val, str) or not date_val:
                    # 如果数据中没有日期，尝试从文件名获取
                    # 这里需要传递文件名信息，暂时使用当前日期
                    date_val = datetime.now().strftime('%#d/%#m/%Y')
                ws.cell(row=r, column=col_idx_map['日期']).value = date_val
                # 直接写入的字段（解析自分组名称的列）
                ws.cell(row=r, column=col_idx_map['总代号']).value = row.get('总代号', '')
                ws.cell(row=r, column=col_idx_map['总代名']).value = row.get('总代名', '')
                ws.cell(row=r, column=col_idx_map['方式']).value = row.get('方式', '')
                ws.cell(row=r, column=col_idx_map['投放方式']).value = row.get('投放方式', '')

                # A-J 中 F-J 用公式从后面的复合字段中提取
                # 提取金额（左半部分）与人数（右半部分），分隔符包含 '|'
                def left_amount_formula(col_letter):
                    # 若包含“|”则取左侧金额，否则若可转数值则取数，否则留空
                    return (
                        f"=IFERROR(IF(ISNUMBER(SEARCH(\"|\",{col_letter}{r})),"
                        f"VALUE(TRIM(LEFT({col_letter}{r},SEARCH(\"|\",{col_letter}{r})-1))),"
                        f"IFERROR(VALUE(TRIM({col_letter}{r})),\"\")),\"\")"
                    )
                def right_count_formula(col_letter):
                    # 若包含“|”则取右侧人数，否则留空；出错留空
                    return (
                        f"=IFERROR(IF(ISNUMBER(SEARCH(\"|\",{col_letter}{r})),"
                        f"VALUE(TRIM(MID({col_letter}{r},SEARCH(\"|\",{col_letter}{r})+1,99))),\"\"),\"\")"
                    )

                # 列字母
                def letter(col_index):
                    return get_column_letter(col_index)

                # 优先写入已解析好的数值列；若不存在则使用公式从复合列拆分
                def set_value_or_formula(col_name, value_series_key, formula):
                    val = row.get(value_series_key, None)
                    if val is not None and val != '' and not (isinstance(val, float) and pd.isna(val)):
                        ws.cell(row=r, column=col_idx_map[col_name]).value = val
                    else:
                        ws.cell(row=r, column=col_idx_map[col_name]).value = formula

                set_value_or_formula('一级首充人数', '一级首充人数', right_count_formula(letter(col_first_charge_lv1)))
                set_value_or_formula('首充金额', '首充金额', left_amount_formula(letter(col_first_charge)))
                set_value_or_formula('首充人数', '首充人数', right_count_formula(letter(col_first_charge)))
                set_value_or_formula('非首充充值金额', '非首充充值金额', left_amount_formula(letter(col_non_first)))
                set_value_or_formula('非首充人数', '非首充人数', right_count_formula(letter(col_non_first)))

                # 剩余列直接写值
                for name in headers[10:]:
                    ws.cell(row=r, column=col_idx_map[name]).value = row.get(name, '')

        daily_channel_data = generate_daily_channel_sheet(daily_q_files, daily_u_files, daily_e_files)
        if daily_channel_data is not None and not daily_channel_data.empty:
            format_daily_channel_sheet(writer, daily_channel_data)
            print("- 'Daily Channel Statistics' sheet written and formatted.")
        else:
            print("  - No daily channel statistics data found")
        
        # 9. 渠道首充复登
        print("\nProcessing 'Channel First Pay Login Retention' sheet...")
        if channel_login_files:  # 使用 recharge_return 文件作为首充复登数据源
            # 获取全平台留存文件中的首充人数数据
            platform_login_files = [f for f in login_return_files if '全平台' in f]
            first_pay_mapping = {}
            if platform_login_files:
                print(f"  Using platform login data for first pay count: {len(platform_login_files)} files")
                # 读取全平台留存文件获取首充人数
                try:
                    platform_df = pd.read_csv(platform_login_files[0], encoding='utf-8')
                    if '首充人数' in platform_df.columns and '时间' in platform_df.columns:
                        first_pay_mapping = dict(zip(platform_df['时间'], platform_df['首充人数']))
                        print(f"  First pay mapping created with {len(first_pay_mapping)} entries")
                    else:
                        first_pay_mapping = {}
                except Exception as e:
                    print(f"  Error reading platform login file: {e}")
                    first_pay_mapping = {}
            else:
                first_pay_mapping = {}
            
            format_channel_retention_sheet(writer, channel_login_files, '渠道首充复登', ['次日', '3日', '7日', '30日'], first_pay_mapping)
            print("- 'Channel First Pay Login Retention' sheet written and formatted.")
        else:
            print("  - No channel login retention files found")
        
        # 10. 渠道首充复充
        print("\n正在处理 '渠道首充复充' sheet...")
        if channel_recharge_files:  # 使用 用户留存率_首充复充 文件
            format_channel_retention_sheet(writer, channel_recharge_files, '渠道首充复充', ['次日', '3日', '7日', '30日'])
            print("- '渠道首充复充' sheet 已写入并格式化。")
        else:
            print("  - No channel recharge retention files found for 渠道首充复充")
        
        # 11. 短信LTV - 按渠道分别去重后按日期合并
        print("\nProcessing 'SMS LTV' sheet...")
        if sms_ltv_files:
            try:
                # Only process files with current or previous month in filename
                filtered_sms_files = [f for f in sms_ltv_files if '本月' in f or '上月' in f]
                print(f"  - Filtered to {len(filtered_sms_files)} files (current and previous month only)")
                
                # Group by channel code
                channel_data = {}
                for file in filtered_sms_files:
                    basename = os.path.basename(file)
                    match = re.search(r'download_ltv_(\d+)', basename)
                    if match:
                        channel_code = match.group(1)
                        if channel_code not in channel_data:
                            channel_data[channel_code] = []
                        
                        print(f"    Processing SMS LTV file for channel {channel_code}")
                    try:
                        df = pd.read_csv(file, encoding='utf-8')
                    except:
                        try:
                            df = pd.read_csv(file, encoding='gbk')
                        except:
                            df = pd.read_csv(file, encoding='latin-1')
                        df.columns = df.columns.str.strip()
                        channel_data[channel_code].append(df)
                
                # Deduplicate each channel separately, then combine
                all_channel_dfs = []
                for channel_code, dfs in channel_data.items():
                    if dfs:
                        channel_df = pd.concat(dfs, ignore_index=True)
                        if '时间' in channel_df.columns:
                            channel_df = channel_df.drop_duplicates(subset=['时间'], keep='last')
                            print(f"  - Channel {channel_code}: {len(channel_df)} rows after deduplication")
                        all_channel_dfs.append(channel_df)
                
                if all_channel_dfs:
                    sms_ltv_combined = pd.concat(all_channel_dfs, ignore_index=True)
                    
                    if '时间' in sms_ltv_combined.columns:
                        # 排除时间列和LTV标签列
                        ltv_label_cols = ['首日LTV', '2日LTV', '3日LTV', '7日LTV', '14日LTV', '30日LTV', '60日LTV']
                        numeric_cols = []
                        
                        for col in sms_ltv_combined.columns:
                            if col != '时间' and col not in ltv_label_cols:
                                try:
                                    sms_ltv_combined[col] = pd.to_numeric(
                                        sms_ltv_combined[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), 
                                        errors='coerce'
                                    )
                                    if sms_ltv_combined[col].dtype in ['int64', 'float64']:
                                        numeric_cols.append(col)
                                except:
                                    pass
                        
                        print(f"  - Found {len(numeric_cols)} numerical columns to sum")
                        if len(numeric_cols) > 0:
                            print(f"  - Sample numeric columns: {numeric_cols[:5]}")
                        
                        if numeric_cols:
                            # 保存非数值列（LTV标签列等）
                            non_numeric_cols = [col for col in sms_ltv_combined.columns if col not in numeric_cols and col != '时间']
                            
                            # Group by time and sum数值列
                            sms_ltv_summed = sms_ltv_combined.groupby('时间', as_index=False)[numeric_cols].sum()
                            
                            # 合并非数值列（取first）
                            if non_numeric_cols:
                                non_numeric_data = sms_ltv_combined.groupby('时间', as_index=False)[non_numeric_cols].first()
                                sms_ltv_combined = pd.merge(sms_ltv_summed, non_numeric_data, on='时间', how='left')
                            else:
                                sms_ltv_combined = sms_ltv_summed
                            
                            print(f"  - After grouping: {len(sms_ltv_combined)} rows, {len(sms_ltv_combined.columns)} columns")
                        else:
                            sms_ltv_combined = sms_ltv_combined.drop_duplicates(subset=['时间'], keep='last')
                            print(f"  - After deduplication: {len(sms_ltv_combined)} rows")
                        
                        sms_ltv_combined = sms_ltv_combined.sort_values('时间')
                    
                    format_ltv_sheet_with_headers(writer, sms_ltv_combined, '短信LTV')
                    print("  - 'SMS LTV' sheet written with formatted headers.")
            except Exception as e:
                print(f"  Error processing SMS LTV files: {e}")
                import traceback
                traceback.print_exc()
        
        # 12. 网红LTV - 按渠道分别去重后按日期合并
        print("\nProcessing 'Influencer LTV' sheet...")
        if influencer_ltv_files:
            try:
                # Only process files with current or previous month in filename
                filtered_influencer_files = [f for f in influencer_ltv_files if '本月' in f or '上月' in f]
                print(f"  - Filtered to {len(filtered_influencer_files)} files (current and previous month only)")
                
                # Group by channel code
                channel_data = {}
                for file in filtered_influencer_files:
                    basename = os.path.basename(file)
                    match = re.search(r'download_ltv_(\d+)', basename)
                    if match:
                        channel_code = match.group(1)
                        if channel_code not in channel_data:
                            channel_data[channel_code] = []
                        
                        print(f"    Processing Influencer LTV file for channel {channel_code}")
                    try:
                        df = pd.read_csv(file, encoding='utf-8')
                    except:
                        try:
                            df = pd.read_csv(file, encoding='gbk')
                        except:
                            df = pd.read_csv(file, encoding='latin-1')
                        df.columns = df.columns.str.strip()
                        channel_data[channel_code].append(df)
                
                # Deduplicate each channel separately, then combine
                all_channel_dfs = []
                for channel_code, dfs in channel_data.items():
                    if dfs:
                        channel_df = pd.concat(dfs, ignore_index=True)
                        if '时间' in channel_df.columns:
                            channel_df = channel_df.drop_duplicates(subset=['时间'], keep='last')
                            print(f"  - Channel {channel_code}: {len(channel_df)} rows after deduplication")
                        all_channel_dfs.append(channel_df)
                
                if all_channel_dfs:
                    influencer_ltv_combined = pd.concat(all_channel_dfs, ignore_index=True)
                    
                    if '时间' in influencer_ltv_combined.columns:
                        # 排除时间列和LTV标签列
                        ltv_label_cols = ['首日LTV', '2日LTV', '3日LTV', '7日LTV', '14日LTV', '30日LTV', '60日LTV']
                        numeric_cols = []
                        
                        for col in influencer_ltv_combined.columns:
                            if col != '时间' and col not in ltv_label_cols:
                                try:
                                    influencer_ltv_combined[col] = pd.to_numeric(
                                        influencer_ltv_combined[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), 
                                        errors='coerce'
                                    )
                                    if influencer_ltv_combined[col].dtype in ['int64', 'float64']:
                                        numeric_cols.append(col)
                                except:
                                    pass
                        
                        print(f"  - Found {len(numeric_cols)} numerical columns to sum")
                        if len(numeric_cols) > 0:
                            print(f"  - Sample numeric columns: {numeric_cols[:5]}")
                        
                        if numeric_cols:
                            # 保存非数值列
                            non_numeric_cols = [col for col in influencer_ltv_combined.columns if col not in numeric_cols and col != '时间']
                            
                            # Group by time and sum数值列
                            influencer_ltv_summed = influencer_ltv_combined.groupby('时间', as_index=False)[numeric_cols].sum()
                            
                            # 合并非数值列
                            if non_numeric_cols:
                                non_numeric_data = influencer_ltv_combined.groupby('时间', as_index=False)[non_numeric_cols].first()
                                influencer_ltv_combined = pd.merge(influencer_ltv_summed, non_numeric_data, on='时间', how='left')
                            else:
                                influencer_ltv_combined = influencer_ltv_summed
                            
                            print(f"  - After grouping: {len(influencer_ltv_combined)} rows, {len(influencer_ltv_combined.columns)} columns")
                        else:
                            influencer_ltv_combined = influencer_ltv_combined.drop_duplicates(subset=['时间'], keep='last')
                            print(f"  - After deduplication: {len(influencer_ltv_combined)} rows")
                        
                        influencer_ltv_combined = influencer_ltv_combined.sort_values('时间')
                    
                    format_ltv_sheet_with_headers(writer, influencer_ltv_combined, '网红LTV')
                    print("  - 'Influencer LTV' sheet written with formatted headers.")
            except Exception as e:
                print(f"  Error processing Influencer LTV files: {e}")
                import traceback
                traceback.print_exc()
        
        # 13. 投放LTV - 按渠道分别去重后按日期合并
        print("\nProcessing 'Ad LTV' sheet...")
        if ad_ltv_files:
            try:
                # Only process files with current or previous month in filename
                filtered_ad_files = [f for f in ad_ltv_files if '本月' in f or '上月' in f]
                print(f"  - Filtered to {len(filtered_ad_files)} files (current and previous month only)")
                
                # Group by channel code
                channel_data = {}
                for file in filtered_ad_files:
                    basename = os.path.basename(file)
                    match = re.search(r'download_ltv_(\d+)', basename)
                    if match:
                        channel_code = match.group(1)
                        if channel_code not in channel_data:
                            channel_data[channel_code] = []
                        
                        print(f"    Processing Ad LTV file for channel {channel_code}")
                    try:
                        df = pd.read_csv(file, encoding='utf-8')
                    except:
                        try:
                            df = pd.read_csv(file, encoding='gbk')
                        except:
                            df = pd.read_csv(file, encoding='latin-1')
                        df.columns = df.columns.str.strip()
                        channel_data[channel_code].append(df)
                
                # Deduplicate each channel separately, then combine
                all_channel_dfs = []
                for channel_code, dfs in channel_data.items():
                    if dfs:
                        channel_df = pd.concat(dfs, ignore_index=True)
                        if '时间' in channel_df.columns:
                            channel_df = channel_df.drop_duplicates(subset=['时间'], keep='last')
                            print(f"  - Channel {channel_code}: {len(channel_df)} rows after deduplication")
                        all_channel_dfs.append(channel_df)
                
                if all_channel_dfs:
                    ad_ltv_combined = pd.concat(all_channel_dfs, ignore_index=True)
                    
                    if '时间' in ad_ltv_combined.columns:
                        # 排除时间列和LTV标签列
                        ltv_label_cols = ['首日LTV', '2日LTV', '3日LTV', '7日LTV', '14日LTV', '30日LTV', '60日LTV']
                        numeric_cols = []
                        
                        for col in ad_ltv_combined.columns:
                            if col != '时间' and col not in ltv_label_cols:
                                try:
                                    ad_ltv_combined[col] = pd.to_numeric(
                                        ad_ltv_combined[col].astype(str).str.replace('$', '').str.replace('%', '').str.replace(',', ''), 
                                        errors='coerce'
                                    )
                                    if ad_ltv_combined[col].dtype in ['int64', 'float64']:
                                        numeric_cols.append(col)
                                except:
                                    pass
                        
                        print(f"  - Found {len(numeric_cols)} numerical columns to sum")
                        if len(numeric_cols) > 0:
                            print(f"  - Sample numeric columns: {numeric_cols[:5]}")
                        
                        if numeric_cols:
                            # 保存非数值列
                            non_numeric_cols = [col for col in ad_ltv_combined.columns if col not in numeric_cols and col != '时间']
                            
                            # Group by time and sum数值列
                            ad_ltv_summed = ad_ltv_combined.groupby('时间', as_index=False)[numeric_cols].sum()
                            
                            # 合并非数值列
                            if non_numeric_cols:
                                non_numeric_data = ad_ltv_combined.groupby('时间', as_index=False)[non_numeric_cols].first()
                                ad_ltv_combined = pd.merge(ad_ltv_summed, non_numeric_data, on='时间', how='left')
                            else:
                                ad_ltv_combined = ad_ltv_summed
                            
                            print(f"  - After grouping: {len(ad_ltv_combined)} rows, {len(ad_ltv_combined.columns)} columns")
                        else:
                            ad_ltv_combined = ad_ltv_combined.drop_duplicates(subset=['时间'], keep='last')
                            print(f"  - After deduplication: {len(ad_ltv_combined)} rows")
                        
                        ad_ltv_combined = ad_ltv_combined.sort_values('时间')
                    
                    format_ltv_sheet_with_headers(writer, ad_ltv_combined, '投放LTV')
                    print("  - 'Ad LTV' sheet written with formatted headers.")
            except Exception as e:
                print(f"  Error processing Ad LTV files: {e}")
                import traceback
                traceback.print_exc()
        
        if 'Sheet1' in writer.book.sheetnames:
            writer.book.remove(writer.book['Sheet1'])
            
    print(f"\n--- 全部处理完成！---")
    print(f"已成功生成或更新文件: '{os.path.basename(output_excel_filename)}'")

if __name__ == "__main__":
    main()
